## ğŸ§  Overview | é¡¹ç›®ç®€ä»‹

**Text2Motion-RL** is a reinforcement learning (RL) based framework designed for generating realistic 3D human motions from natural language instructions. Unlike traditional supervised learning models, our approach leverages the exploration ability of RL to optimize motion quality based on imitation, smoothness, and semantic rewards.

**Text2Motion-RL** æ˜¯ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ–‡æœ¬é©±åŠ¨äººä½“åŠ¨ä½œç”Ÿæˆæ¡†æ¶ï¼Œèƒ½æ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤ç”Ÿæˆé«˜è´¨é‡çš„3DåŠ¨ä½œã€‚è¯¥æ–¹æ³•ä¸ä¾èµ–å…¨ç›‘ç£æ•°æ®ï¼Œè€Œæ˜¯é€šè¿‡ç­–ç•¥ä¼˜åŒ–ç›´æ¥å¯¹è¿åŠ¨è´¨é‡è¿›è¡Œä¼˜åŒ–ã€‚

---

## ğŸš€ Features | é¡¹ç›®äº®ç‚¹

- ğŸ¯ Text-conditioned motion generation
- â™»ï¸ Multi-objective reward: imitation, smoothness, semantics
- ğŸ“¡ PPO-based actor-critic architecture
- ğŸ§© Support for HumanML3D dataset
- ğŸ¥ Training progress & motion visualization

---

## ğŸ“¦ Project Structure | é¡¹ç›®ç»“æ„

```bash
text2motion_rl/
â”œâ”€â”€ train/                  # è®­ç»ƒè„šæœ¬
â”‚   â””â”€â”€ train_ppo.py
â”œâ”€â”€ envs/                   # è‡ªå®šä¹‰ç¯å¢ƒ
â”‚   â””â”€â”€ text2motion_env.py
â”œâ”€â”€ models/                 # æ¨¡å‹æ¶æ„
â”‚   â”œâ”€â”€ actor_critic.py
â”‚   â””â”€â”€ policy_wrapper.py
â”œâ”€â”€ rewards/                # å¥–åŠ±å‡½æ•°
â”‚   â””â”€â”€ reward_functions.py
â”œâ”€â”€ utils/                  # å·¥å…·å‡½æ•°
â”‚   â””â”€â”€ logger.py
â”œâ”€â”€ data/                   # ç‰¹å¾æ•°æ®
â”‚   â”œâ”€â”€ image_features.npy
â”‚   â”œâ”€â”€ text_features.npy
â”‚   â””â”€â”€ action_labels.npy
â””â”€â”€ README.md
```

---

## ğŸ§© Model Architecture | æ¨¡å‹ç»“æ„

```bash
Text Instruction â†’ Policy Ï€ â†’ Action Sequence
                        â†“
             [Reward from Environment]
         â†‘     â†‘         â†‘
     imitation  smoothness  semantic
```

We use an actor-critic architecture where the policy outputs 263D continuous action vectors. Observations include previous actions and text features.

---

## ğŸ› ï¸ Current Issues & Suggestions | å½“å‰é—®é¢˜ä¸ä¼˜åŒ–å»ºè®®

### â— ç°æœ‰é—®é¢˜
- è®­ç»ƒè€—æ—¶é•¿ï¼šæ¯è½®è®­ç»ƒè€—æ—¶è¾¾ 600s+ï¼ŒGPU ä½¿ç”¨æ•ˆç‡ä¸é«˜
- å¥–åŠ±å‡½æ•°è¿‡æ…¢ï¼šå½“å‰ reward å‡½æ•°åŸºäº NumPy å’Œ fastdtwï¼Œæ•ˆç‡è¾ƒä½
- Value Loss æ³¢åŠ¨å¤§ï¼šè¯´æ˜ critic ç½‘ç»œå­¦ä¹ ä¸ç¨³å®š
- Motion ç»´åº¦å¤„ç†ä¸ä¸€è‡´ï¼šGT ç»´åº¦æœ‰æ—¶ä¸º 195ï¼Œä¸ action ä¸åŒ¹é…
- buffer æ•°æ®æœªæ ‡å‡†åŒ–ï¼Œå½±å“ç¨³å®šæ€§

### âœ… ä¼˜åŒ–å»ºè®®
- âœ… ä½¿ç”¨ `torch` é‡å†™ reward å‡½æ•°ï¼Œæ›¿ä»£ `numpy+fastdtw`
- âœ… æ”¹ä¸º `generated_motion = np.zeros([max_len, act_dim])` æå‰åˆ†é…å†…å­˜
- âœ… å¯¹ `obs`, `action`, `reward` åšæ ‡å‡†åŒ–
- âœ… å¾®è°ƒ critic å­¦ä¹ ç‡å¹¶å¼•å…¥ reward clipping
- âœ… å¼•å…¥ motion encoder åšè¯­ä¹‰å¥–åŠ±ï¼ˆå¯é€‰ï¼‰

---

## ğŸ“š Dataset | æ•°æ®é›†


HEAD
We use the [HumanML3D](https://github.com/EricGuo5513/HumanML3D) dataset, which provides text-action pairs with aligned 3D pose sequences.
=======


---

## ğŸ“ˆ Training Logs | è®­ç»ƒæ—¥å¿—æ ·ä¾‹

```bash
Epoch 0053 | Avg Reward: -329.79 | Policy Loss: 0.6477 | Value Loss: 117.6077
Epoch 0054 | Avg Reward: -328.25 | Policy Loss: 0.7786 | Value Loss: 193.6854
```

---

## ğŸ“¦ Dependencies | ä¾èµ–ç¯å¢ƒ

- Python 3.8+
- PyTorch 1.10+
- NumPy, tqdm, fastdtw
- HumanML3D preprocessing

---

## ğŸ“§ Contact | è”ç³»æ–¹å¼

Please contact `kkuo4682@gmail.com` for any questions about this project.

---

**Enjoy motion generation with RL! | å¼ºåŒ–å­¦ä¹ è®©åŠ¨ä½œæ›´æ™ºèƒ½ï¼**
